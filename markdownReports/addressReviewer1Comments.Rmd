---
title: "Synch Manuscript Parameter Assumptions"
author: "Cam Freshwater"
date: "April 25, 2019"
output: html_document
---

This document addresses two reviewer comments on the synchrony simulation model.

1. Comment one asked why we used Smax as the mean for the prior for beta when it is possible that observed spawner abundance has exceeded that. To address fit SR models (stored in singleStock repo) using various caps on the mean beta prior (Smax; 3Smax) and various uncertainties (sigma = 1; 3). Fit only for Ricker stocks and provide means and SD's 


```{r importAndFit, eval=FALSE, include=FALSE}
require(here); require(R2jags); require(ggplot2); require(tidyverse); require(samSim)

source(here("scripts/func/fitRicker.R"))

# Clean data to match format for stockRecFunctions.R
# Use total spawners
srDatTrim <- read.csv(here("data/sox/fraserRecDat.csv"), 
                  stringsAsFactors=F) %>% 
  select(stk, stkName, yr, totalSpwn, rec) %>% 
  rename(CU = stk, Year = yr, Escape = totalSpwn, Recruit = rec) %>% 
  filter(!(stkName == "Cultus" & Year > 1999)) %>% # trim Cultus
  mutate(lnRS = log(Recruit/Escape))

betaMuScalars <- rep(c(1, 2), times = 2)
betaSD <- rep(c(1, 5), each = 2)
fileNames <- paste("mu", betaMuScalars, "_sd", betaSD, sep = "")

# for (i in seq_along(betaMuScalars)) {
#   RBayesSingle(dat = srDatTrim, Niter = 250000, Nthin = 50, burnin = 50000,
#                fname = fileNames[i], smaxMuScalar = betaMuScalars[i], 
#                smaxPriorSD = betaSD[i],
#                cap = 3, tauPrior = 0.001) 
# }

```

```{r gatherDataAndPlot}
parList <- lapply(seq_along(fileNames), function(i) {
  pars <- read.csv(here("outputs/srAnalysis/data", 
                        paste(fileNames[i],"MCMCPars.csv", sep = "")))
  modelSumm <- readRDS(here("outputs/srAnalysis/data", 
                            paste(fileNames[i],"SimOutput.RDS", sep = "")))
  pars <- pars %>% 
    mutate(muPriorScalar = as.factor(betaMuScalars[i]),
           sdPrior = as.factor(betaSD[i]),
           file = as.factor(fileNames[i]),
           stk = as.factor(abbreviate(stk, minlength = 4)))
  tList <- list(pars, modelSumm)
  names(tList) <- c("pars", "modelSummary")
  return(tList)
})

wideParDat <- do.call(rbind, lapply(parList, function(x) x$pars))

parDat <- wideParDat %>% 
  gather(key = parameter, value = value, -CU, -stk, -muPriorScalar, -sdPrior,
         -file)
# write.csv(parDat, here("outputs", "srAnalysis",
#                        "ricker_betaPriors_mcmcOut.csv"))

parDat <- read.csv(here("outputs", "srAnalysis", "ricker_betaPriors_mcmcOut.csv"))

betaDat <- parDat %>% 
  filter(parameter == "beta0")

# png(here("outputs/srAnalysis/betaPosterior_capsTauPriors.png"), units = "in", 
#     height = 4.5, width = 6.5, res = 250)
ggplot(betaDat, aes(x = as.factor(muPriorScalar), y = value, 
                    fill = as.factor(sdPrior))) +
  labs(x = "Scalar on Mu of sMax Prior", y = "Beta Estimate", 
       fill = "SD of sMax Prior") +
  geom_boxplot(position = position_dodge(width = 0.65)) +
  theme_sleekX() +
  facet_wrap(~stk, scales = "free_y")
# dev.off()

```

Impacts of priors ranges from negligible to strong. Typically sigma has a much stronger impact than a scalar on mu, but not always. 

2. Comment two asked why we used log-normal distributions to generate stochasticity in en route mortality rather than binomial. To address try to parameterize each distribution using available en route mortality data, then sample 1500 times from each and calculate realized. See whether or not they converge. 

```{r mortData}
require(here); require(tidyverse); require(ggplot2)
cuPar <- read.csv(here("data/sox/fraserCUpars.csv"), stringsAsFactors = F)

enRouteSig <- cuPar$sdDBE
enRouteMR <- cuPar$meanDBE
nCU <- length(unique(cuPar$stk))

# migMortErr <- exp(qnorm(runif(nCU, 0.0001, 0.9999), 0, enRouteSig))
qnorm(runif(nCU, 0.0001, 0.9999), 0, enRouteSig)
enRouteMR * migMortErr

```