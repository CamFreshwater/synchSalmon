---
title: "Realized Productivity Among OMs"
author: "Cam Freshwater"
date: "1/9/2019"
output: html_document
---

```{r readLibraries, message=FALSE, warning=FALSE, include=FALSE}
listOfPackages <- c("plyr", "here", "parallel", "doParallel", "foreach", 
                    "reshape2", "tidyverse", "gsl", "tictoc", "stringr", 
                    "synchrony", "zoo", "Rcpp", "RcppArmadillo", "sn", 
                    "sensitivity", "mvtnorm", "forcats", "ggpubr", "viridis", 
                    "samSim")

here <- here::here

newPackages <- listOfPackages[!(listOfPackages %in% 
                                  installed.packages()[ , "Package"])]
if(length(newPackages)) install.packages(newPackages)
lapply(listOfPackages, require, character.only = TRUE)
```


```{r importAndCleanData, warning=FALSE, include=FALSE}
## PRELIMINARY ANALYSIS IGNORE ##

#Carrie wants to make sure that the skewed (and perhaps reference) productivity #operating models accurately reflect recent low levels of R/S. Ideally this would  be #done by fitting kalman filter stock-recruit models to data generated from skewed OMs #and making sure that alpha values are similar. As a first pass however, simply compare #average R/S during sim period for each CU to observed.

#
```

```{r plotDataPrelim, warning=FALSE, include=FALSE}
# omNames <- c("ref", "skew", "skewT")
# nPrime <- bigCUList[[1]][[1]]$nPrime
# plotDat <- NULL
# for (i in seq_along(dirNames)) {
#   dum <- bigCUList[[i]][[2]]$logRS %>% 
#     melt() %>% 
#     dplyr::rename("yr" = "Var1", "cu" =  "Var2", "trial" = "Var3", 
#                   "logRS" = "value") %>% 
#     group_by(cu, yr) %>% 
#     summarise(meanLogRS = mean(logRS)) %>% 
#     #use means AMONG TRIALS (i.e. violin plots show interannual variability)
#     mutate(om = as.factor(ifelse(yr > nPrime, omNames[i], "obs"))) %>% 
#     #remove all years from priming period except last 2 generations
#     filter(yr > (nPrime - 7))
#   plotDat <- rbind(plotDat, dum)
# }  
# 
# ggplot(plotDat, aes(x = om, y = meanLogRS)) +
#   geom_violin(draw_quantiles = c(0.5), 
#               position = position_dodge(width = 0.75)) +
#   labs(y = "Realized Productivity",
#        x = "Operating Model") +
#   theme_sleekX(facetSize = 1.2, axisSize = 16, legendSize = 0.85) +
#   facet_wrap(~cu, scales = "free_y")

# Variability in productivity in recent years makes it difficult to interpret. Will need to directly compare simulated R/S from alpha values equivalent to Sue's Kalman filter estimates and see how they compare rather than using observed alone. 

```

Carrie supplied alphas estimated using a Kalman filter for 11 Fraser CUs (all Ricker type stocks). Added these to a new fraserCUPars.csv which can be run by samSim without ricPars/larkPars to forward simulate dynamics if productivity remains depressed (see `if` statement in following snippet that is currently commented out).

Note that to avoid issues with overlap constraints (requires all MUs to be present), the MP is a fixed ER (0.40) rather than TAM. This will obviously impact the long-term outcomes but shouldn't influence the comparison between different productivity OMs.

```{r runSimulations, echo=TRUE, warning=FALSE}
simPar <- read.csv(here("data/sox/fraserOMInputs_kalmanA.csv"), 
                   stringsAsFactors = F)
cuPar <- read.csv(here("data/sox/fraserCUpars_lowKalmanA.csv"), 
                  stringsAsFactors = F)
srDat <- read.csv(here("data/sox/fraserRecDatTrim.csv"), stringsAsFactors = F)
catchDat <- read.csv(here("data/sox/fraserCatchDatTrim.csv"), 
                     stringsAsFactors = F)
tamFRP <- read.csv(here("data/sox/tamRefPts.csv"), stringsAsFactors = F)
ricPars <- read.csv(here("data/sox/pooledRickerMCMCPars.csv"), 
                    stringsAsFactors = F)
larkPars <- read.csv(here("data/sox/pooledLarkinMCMCPars.csv"), 
                     stringsAsFactors = F)

simsToRun <- split(simPar, seq(nrow(simPar)))
dirName <- "kalmanA_sockeye"
nTrials <- 150

## CODED OUT UNLESS DATA UNAVAILABLE BECAUSE SIMULATIONS HAVEN'T BEEN RUN LOCALLY
# Ncores <- detectCores()
# cl <- makeCluster(Ncores - 3) #save two cores
# registerDoParallel(cl)
# clusterEvalQ(cl, c(library(MASS),
#                 library(here),
#                 library(sensitivity),
#                 library(mvtnorm),
#                 library(scales), #shaded colors for figs
#                 library(viridis), #color blind gradient palette
#                 library(gsl),
#                 library(dplyr),
#                 library(Rcpp),
#                 library(RcppArmadillo),
#                 library(sn),
#                 library(samSim)))
# #export custom function and objects
# clusterExport(cl, c("simsToRun", "recoverySim", "cuPar", "nTrials", "dirName",
#                     "catchDat", "srDat", "ricPars", "larkPars",
#                     "tamFRP"), envir = environment())
# tic("run in parallel")
# parLapply(cl, simsToRun, function(x) {
#   if (x$nameOM == "lowKalmanA") {
#     # separate out lowKalmaA scenario so that specific alpha values are passed
#     # rather than sampling from posterior distributions
#     recoverySim(x, cuPar, catchDat = catchDat, srDat = srDat,
#                 variableCU = FALSE, ricPars = NULL, larkPars = NULL,
#                 tamFRP = tamFRP, dirName=dirName, nTrials = 150,
#                 multipleMPs = TRUE)
#   } else {
#     recoverySim(x, cuPar, catchDat = catchDat, srDat = srDat,
#                 variableCU = FALSE, ricPars = ricPars, larkPars = larkPars,
#                 tamFRP = tamFRP, dirName=dirName, nTrials = 150,
#                 multipleMPs = TRUE)
#   }
# })
# stopCluster(cl) #end cluster
# toc()
```


```{r prepOutputData, echo=TRUE, warning=FALSE}
subDirNames <- list.files(paste(here("outputs/simData"), dirName, sep="/"))
arrayNames <- sapply(subDirNames, function(x) {
  # list.files(paste(here("outputs/simData"), dirName, x, sep="/"), 
  #            pattern = "\\Arrays.RData$")
    list.files(paste(here("outputs/simData"), dirName, x, sep="/"), 
             pattern = "\\cuDat.RData$")
})
bigCUList <- lapply(seq_along(arrayNames), function (h) {
  datList <- readRDS(paste(here("outputs/simData"), dirName, subDirNames[h],
                             arrayNames[h], sep = "/"))
}) #iterate across different scenarios
names(bigCUList) <- subDirNames

omNames <- subDirNames
nPrime <- bigCUList[[1]]$nPrime
dumFull <- NULL
for (i in seq_along(subDirNames)) {
  # dum1 <- bigCUList[[i]]$logRS %>% 
  #   melt() %>% 
  #   dplyr::rename("yr" = "Var1", "cu" =  "Var2", "trial" = "Var3", 
  #                 "logRS" = "value") %>% 
  #   #remove non-simulation data
  #   filter(yr > nPrime) %>% 
  #   mutate(om = omNames[i])  
  dum1 <- bigCUList[[i]]$medEstAlpha %>% 
    melt() %>% 
    dplyr::rename("trial" = "Var1", "cu" =  "Var2", "estA" = "value") %>% 
    mutate(om = omNames[i]) 
  dumFull <- rbind(dumFull, dum1)
}  

dumFull <- dumFull %>% 
  mutate(om = recode(om, "reference" = "ref", "lowKalmanA" = "low", 
                .default = levels(om)),
         cu = recode(cu, "1" = "Stel",  "2" = "Bowr",  "3" = "Raft", 
                    "4" = "Chlk", "5" = "Birk", "6" = "Cult", "7" = "Port",
                    "8" = "Weav", "9" = "Fenn", "10" = "Nadi", "11" = "Pitt",
                    "12" = "Harr")) %>% 
  mutate(om = factor(om, levels = c("ref", "low", "skew", "skewT")))

## not necessary when using estimated alpha
# means among years
# annualMeans <- dumFull %>% 
#   group_by(cu, trial, om) %>%
#   summarise(meanLogRS = mean(logRS))  
# ## means among trials
# trialMeans <- dumFull %>% 
#   group_by(cu, yr, om) %>%
#   summarise(meanLogRS = mean(logRS))
```

```{r plotProd, echo=TRUE, warning=FALSE}
ggplot(dumFull, aes(x = om, y = estA)) +
  geom_violin(draw_quantiles = c(0.5), 
              position = position_dodge(width = 0.75)) +
  labs(y = "Median Estimated Alpha During Simulation",
       x = "Operating Model") +
  theme_sleekX(facetSize = 1.1, axisSize = 10, legendSize = 0.85) +
  facet_wrap(~cu, scales = "free_y")

```

The plot now shows the median alpha estimated internally by the closed-loop model during the simulation period. The violin plots represent inter-trial variability. There seems to be even closer convergence between the low A and skewed normal estimates, likely due to Carrie's hypothesis that density dependent differences were introducing additional CU-specific variation in the original plots. 

That being said there are a few exceptions, which are basically driven by the relative decline in productivity each CU has experienced (as estimated by a Kalman filter model). For example, Harrison, the largest outlier, has experienced a recent *increase* in productivity. In other words, the "low" OM  here is a misnomer and there is no reason to expect that a pessimistic OM should match the recent kalman filter estimates because the current outlook for Harrison is optimistic. Similarly, other CUs that don't show a consistent trend between the low and skewed N OMs have not exhibited a strong decline in productivity (e.g. Chilko, Nadina). As a result their "low" estimates are similar to the reference scenario because productivity has remained more or less stationary.

In terms of how we move forward I think we need to decide whether the goal of the low productivity scenario is to generate reasonable estimates of a uniform decline within a hypothetical aggregate (with Fraser as a case study) or if its intended to realistically represent the most recent trends in productivity of each Fraser CU. If it's the former I think that the skewed normal estimates are defensible, if the latter I can look into tuning each CU's individual skewness parameter (it's straightforward to pass them as a vector within the model, but will be time consuming to estimate each parameter). 

Personally I would lean towards the first option because I view the alternative OMs as a means of examining the interaction between aggregate variability and productivity rather than a way to predict future trends in the aggregate's dynamics, but I'd like to hear other thoughts too!