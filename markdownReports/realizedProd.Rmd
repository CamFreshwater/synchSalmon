---
title: "Realized Productivity Among OMs"
author: "Cam Freshwater"
date: "1/9/2019"
output: html_document
---

```{r readLibraries, message=FALSE, warning=FALSE, include=FALSE}
listOfPackages <- c("plyr", "here", "parallel", "doParallel", "foreach", 
                    "reshape2", "tidyverse", "gsl", "tictoc", "stringr", 
                    "synchrony", "zoo", "Rcpp", "RcppArmadillo", "sn", 
                    "sensitivity", "mvtnorm", "forcats", "ggpubr", "viridis", 
                    "samSim")

here <- here::here

newPackages <- listOfPackages[!(listOfPackages %in% 
                                  installed.packages()[ , "Package"])]
if(length(newPackages)) install.packages(newPackages)
lapply(listOfPackages, require, character.only = TRUE)
```


```{r importAndCleanData, warning=FALSE, include=FALSE}
## PRELIMINARY ANALYSIS IGNORE ##

#Carrie wants to make sure that the skewed (and perhaps reference) productivity #operating models accurately reflect recent low levels of R/S. Ideally this would  be #done by fitting kalman filter stock-recruit models to data generated from skewed OMs #and making sure that alpha values are similar. As a first pass however, simply compare #average R/S during sim period for each CU to observed.

#
```

```{r plotDataPrelim, warning=FALSE, include=FALSE}
# omNames <- c("ref", "skew", "skewT")
# nPrime <- bigCUList[[1]][[1]]$nPrime
# plotDat <- NULL
# for (i in seq_along(dirNames)) {
#   dum <- bigCUList[[i]][[2]]$logRS %>% 
#     melt() %>% 
#     dplyr::rename("yr" = "Var1", "cu" =  "Var2", "trial" = "Var3", 
#                   "logRS" = "value") %>% 
#     group_by(cu, yr) %>% 
#     summarise(meanLogRS = mean(logRS)) %>% 
#     #use means AMONG TRIALS (i.e. violin plots show interannual variability)
#     mutate(om = as.factor(ifelse(yr > nPrime, omNames[i], "obs"))) %>% 
#     #remove all years from priming period except last 2 generations
#     filter(yr > (nPrime - 7))
#   plotDat <- rbind(plotDat, dum)
# }  
# 
# ggplot(plotDat, aes(x = om, y = meanLogRS)) +
#   geom_violin(draw_quantiles = c(0.5), 
#               position = position_dodge(width = 0.75)) +
#   labs(y = "Realized Productivity",
#        x = "Operating Model") +
#   theme_sleekX(facetSize = 1.2, axisSize = 16, legendSize = 0.85) +
#   facet_wrap(~cu, scales = "free_y")

# Variability in productivity in recent years makes it difficult to interpret. Will need to directly compare simulated R/S from alpha values equivalent to Sue's Kalman filter estimates and see how they compare rather than using observed alone. 

```

Carrie supplied alphas estimated using a Kalman filter for 11 Fraser CUs (all Ricker type stocks). Added these to a new fraserCUPars.csv which can be run by samSim without ricPars/larkPars to forward simulate dynamics if productivity remains depressed (see `if` statement in following snippet that is currently commented out).

Note that to avoid issues with overlap constraints (requires all MUs to be present), the MP is a fixed ER (0.40) rather than TAM. This will obviously impact the long-term outcomes but shouldn't influence the comparison between different productivity OMs.

```{r runSimulations, echo=TRUE, warning=FALSE}
simPar <- read.csv(here("data/sox/fraserOMInputs_kalmanA.csv"), 
                   stringsAsFactors = F)
cuPar <- read.csv(here("data/sox/fraserCUpars_lowKalmanA.csv"), 
                  stringsAsFactors = F)
srDat <- read.csv(here("data/sox/fraserRecDatTrim.csv"), stringsAsFactors = F)
catchDat <- read.csv(here("data/sox/fraserCatchDatTrim.csv"), 
                     stringsAsFactors = F)
tamFRP <- read.csv(here("data/sox/tamRefPts.csv"), stringsAsFactors = F)
ricPars <- read.csv(here("data/sox/pooledRickerMCMCPars.csv"), 
                    stringsAsFactors = F)
larkPars <- read.csv(here("data/sox/pooledLarkinMCMCPars.csv"), 
                     stringsAsFactors = F)

simsToRun <- split(simPar, seq(nrow(simPar)))
dirName <- "kalmanA_sockeye"

## CODED OUT UNLESS DATA UNAVAILABLE BECAUSE SIMULATIONS HAVEN'T BEEN RUN LOCALLY
# Ncores <- detectCores()
# cl <- makeCluster(Ncores - 3) #save two cores
# registerDoParallel(cl)
# clusterEvalQ(cl, c(library(MASS),
#                 library(here),
#                 library(sensitivity),
#                 library(mvtnorm),
#                 library(scales), #shaded colors for figs
#                 library(viridis), #color blind gradient palette
#                 library(gsl), 
#                 library(dplyr),
#                 library(Rcpp),
#                 library(RcppArmadillo),
#                 library(sn),
#                 library(samSim)))
# #export custom function and objects
# clusterExport(cl, c("simsToRun", "recoverySim", "cuPar", "nTrials", "dirName",
#                     "catchDat", "srDat", "ricPars", "larkPars", 
#                     "tamFRP"), envir = environment()) 
# tic("run in parallel")
# parLapply(cl, simsToRun, function(x) {
#   if (x$nameOM == "lowKalmanA") {
#     # separate out lowKalmaA scenario so that specific alpha values are passed 
#     # rather than sampling from posterior distributions
#     recoverySim(x, cuPar, catchDat = catchDat, srDat = srDat, 
#                 variableCU = FALSE, ricPars = NULL, larkPars = NULL, 
#                 tamFRP = tamFRP, dirName=dirName, nTrials = 150,
#                 multipleMPs = TRUE)
#   } else {
#     recoverySim(x, cuPar, catchDat = catchDat, srDat = srDat, 
#                 variableCU = FALSE, ricPars = ricPars, larkPars = larkPars, 
#                 tamFRP = tamFRP, dirName=dirName, nTrials = 150,
#                 multipleMPs = TRUE)
#   }
# })
# stopCluster(cl) #end cluster
# toc()
```


```{r prepOutputData, echo=TRUE, warning=FALSE}
subDirNames <- list.files(paste(here("outputs/simData"), dirName, sep="/"))
arrayNames <- sapply(subDirNames, function(x) {
  list.files(paste(here("outputs/simData"), dirName, x, sep="/"), 
             pattern = "\\Arrays.RData$")
})
bigCUList <- lapply(seq_along(arrayNames), function (h) {
  datList <- readRDS(paste(here("outputs/simData"), dirName, subDirNames[h],
                             arrayNames[h], sep = "/"))
}) #iterate across different scenarios
names(bigCUList) <- subDirNames

omNames <- subDirNames
nPrime <- bigCUList[[1]]$nPrime
dumFull <- NULL
for (i in seq_along(subDirNames)) {
  dum1 <- bigCUList[[i]]$logRS %>% 
    melt() %>% 
    dplyr::rename("yr" = "Var1", "cu" =  "Var2", "trial" = "Var3", 
                  "logRS" = "value") %>% 
    #remove non-simulation data
    filter(yr > nPrime) %>% 
    mutate(om = omNames[i])  
  dumFull <- rbind(dumFull, dum1)
}  

dumFull <- dumFull %>% 
  mutate(om = recode(om, "reference" = "ref", "lowKalmanA" = "low", 
                .default = levels(om)),
         cu = recode(cu, "1" = "Stel",  "2" = "Bowr",  "3" = "Raft", 
                    "4" = "Chlk", "5" = "Birk", "6" = "Cult", "7" = "Port",
                    "8" = "Weav", "9" = "Fenn", "10" = "Nadi", "11" = "Pitt",
                    "12" = "Harr")) %>% 
  mutate(om = factor(om, levels = c("ref", "low", "skew", "skewT")))

# means among years
annualMeans <- dumFull %>% 
  group_by(cu, trial, om) %>%
  summarise(meanLogRS = mean(logRS))  
## means among trials
trialMeans <- dumFull %>% 
  group_by(cu, yr, om) %>%
  summarise(meanLogRS = mean(logRS))
```

```{r plotProd, echo=TRUE, warning=FALSE}
#violin plots among years
ggplot(trialMeans, aes(x = om, y = meanLogRS)) +
  geom_violin(draw_quantiles = c(0.5), 
              position = position_dodge(width = 0.75)) +
  labs(y = "Realized Mean Productivity By Year",
       x = "Operating Model") +
  theme_sleekX(facetSize = 1.1, axisSize = 10, legendSize = 0.85) +
  facet_wrap(~cu, scales = "free_y")

#violin plots among trials
ggplot(annualMeans, aes(x = om, y = meanLogRS)) +
  geom_violin(draw_quantiles = c(0.5), 
              position = position_dodge(width = 0.75)) +
  labs(y = "Realized Mean Productivity By Trial",
       x = "Operating Model") +
  theme_sleekX(facetSize = 1.1, axisSize = 10, legendSize = 0.85) +
  facet_wrap(~cu, scales = "free_y")

```

First plot shows realized mean productivity by year (i.e. violin plots show interannual variability), while the second shows means by trial (i.e. violin plots show intertrial variability). Non-summarized data have too much variability to interpret easily. Variability among CUs means that skewed normal drifts from low Kalman alpha estimates in certain cases, particularly when looking at interannual rather than intertrial medians. Overall though, both OMs have approximately similar realized productivity. As the model is currently set up, it's not really possible to provide individual skewness parameters for each CU so I'm not sure what the next course of action is for tuning. 

One final thing to note is that we should definitely double check that nothing funky happens with the Larkin CUs. 